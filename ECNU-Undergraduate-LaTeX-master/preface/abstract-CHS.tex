\thispagestyle{empty}
\input{paper_info.tex}
\centerline{\bfseries \sffamily \zihao{-3}\TitleCHS}
\renewcommand\abstractname{\sffamily\zihao{-4} 摘要}
\begin{abstract}\zihao{5}\rmfamily
	\par 本文主要针对Nvidia新架构的GPU（图灵架构）为机器学习应用带来的性能提升进行研究，由于目前实际使用中的应用很难达到Nvidia官方宣传的性能提升幅度，故本文将从问题类型、代码结构结合硬件、指令特征对这一现象进行研究，并提出相应的建议。本文主要采用定量方法，通过不同世代的硬件和SDK进行横向比较，以及同一世代硬件、SDK和不同类型应用进行纵向比较；并总结出特征。在研究中较为重要的部分为新硬件中加入的张量核心（Tensor Core）以及对应的线性代数库CUTLASS，文章将通过混合矩阵运算、矩阵乘法、卷积运算等对其进行评估；其他还涉及了传统的矩阵运算库CUBLAS、模型优化器TensorRT以及最为基本的浮点计算、内存种类等。
	\par 根据实验结果，新架构硬件中张量核心对于机器学习应用的类型、计算类型、超参数等条件敏感；要达到期望的性能，输入数据规模、形状、运算占比等方面有较为严苛的需求；在矩阵较为稀疏、输入规模较小时CUSPARSE稀疏矩阵库和基于纹理内存的方法能取得更高性能；而计算输入较为规律、符合硬件形状时张量核心能带来显著提升。至于网络推理阶段，TensorRT在各种情况下均能带来明显的提升。在实际应用中，训练阶段应根据任务特征合理选择硬件、SDK和内存系统使用；而在推理阶段应利用Tensor Core提升吞吐量。
	\newline
	\newline
	{\bfseries \sffamily\zihao{5} 关键词：} \zihao{5}{\rmfamily \KeywordsCHS}
\end{abstract}