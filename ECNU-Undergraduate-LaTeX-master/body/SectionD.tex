\newpage
\section{总结与展望}
\setcounter{table}{0}
\setcounter{figure}{0}
\subsection{总结}
\par 本文主要通过自底向上的方式对NVIDIA最近发布的新架构硬件(伏特、图灵架构)在机器学习应用中的性能提升进行了评估，其中在新架构中新加入的张量核心(Tensor Core)是本文考察的重点，并根据评估结果给出了编程建议，以及对于下一代硬件的一些合理设想。
\par 在正式评估之前，本文对基于CUDA的GPU编程模型进行了简要介绍，其中包含CUDA应用程序的编写步骤、调用方式、内存模型等，同时还对中间层的PTX代码进行了简要介绍，这些知识对于后文性能分析部分有较大的帮助。接下来本文便从三个层级对基于CUDA的机器学习应用在新架构硬件上的性能提升幅度进行了评估。
\par 首先，本文涉及的最低层面便是用途单一、专为评估绝对性能设计的简单应用进行基准测试，这些应用大部分是NVIDIA官方发布的测试用例。这些测试用例涵盖混合矩阵运算(GEMM)、矩阵乘法、卷积核神经网络推理。根据实验得到的结果，在混合矩阵运算(GEMM)方面，新架构硬件能在操作数形状、尺寸与硬件参数、调用特征较为匹配的情况下取得大幅度的性能提升，而这些显著的性能提升是采用“用精度换速度”的策略，计算时数据精度均为FP16/INT8等低于传统的精度；然而在不匹配的情况下，性能下降极为明显。在矩阵乘法方面，我们评估了cuBLAS在新架构上的性能提升，结果是在所有情况下使用cuBLAS进行矩阵乘法运算优势都极为明显，且不依赖于操作数的形状、尺寸。在卷积运算方面，基于混合矩阵运算的卷积计算在新架构上相对于原有的基于快速傅里叶变换的计算方法在大规模输入时提升明显，且精度更高；然而在输入规模较小时，使用纹理内存进行直接计算占绝对优势。考虑到目前许多神经网络中的卷积计算的图像规模多为$ 10^1 $-$ 10^2 $数量级，在该数量级上使用纹理内存进行直接计算的方法性能较强，实际应用中应考虑这种方法。以上三种大多是在网络训练阶段涉及的计算，而在网络推理阶段，本文尝试了一种新的模式，即使用TensorRT对训练好的网络结构进行优化并在目标硬件上进行推理。尽管TensorRT目前仅能运行于特定硬件，但是根据本文的实验，使用TensorRT能为网络推理带来极大的吞吐量提升。
\par 在完成基准测试之后，本文移步基于CUDA源码构建的机器学习应用。在该部分中本文分为深度学习应用和传统机器学习应用。深度学习应用选用了结构较为简单的卷积神经网络，而传统机器学习应用选择了支持向量机。在卷积神经网络部分，本文将计算分为前向传播，反向传播更新连接参数，反向传播更新卷积核参数三个部分，分别考察新架构对于这三个部分的提升幅度。实验结果令人意外，除去前向传播中新架构能带来30\%-50\%的提升外，另两个部分中开启新架构甚至会降低性能。其原因为反向传播部分多为梯度计算，能使用混合矩阵计算(GEMM)从而利用到Tensor Core的部分较少，而开启Tensor Core又会对调度、同步、访存和其他指令的发射带来影响，故反向部分会造成性能下降。而前向传播部分由于卷积核、步进、填充的存在，无法保证每一层操作数的形状都能适用于Tensor Core，故提升幅度极为有限。值得注意的是，通过将卷积操作更换为第二章中提到的纹理内存方式，总体性能得到了一定的提升。在支持向量机部分，本文则根据支持向量机输入矩阵较为稀疏的特征，分别考察了使用专为稀疏矩阵设计的API和使用Tensor Core的API进行评估，实验结果表明在数据量较大时，特征矩阵会愈稀疏，专为稀疏矩阵设计的API性能较好，而数据量较小时，仍然是使用Tensor Core的API性能较好。
\par 之后，本文对基于 -GPU框架的应用进行评估。在这一部分我们搭建了一个简单的基于Tensor Flow-GPU的卷积神经网络，目的在于考察在最贴近真实应用场景时如何尽可能利用新硬件提升性能。本文从神经网络的超参数、网络结构、卷积计算方式、数据精度和推理等方面进行考察；结果发现增加网络的批大小能带来显著的训练速度提升，但是过大的批会导致网络准确度下降，实际应用中应权衡这两点；而由于输入的图片尺寸较小，本文通过修改Tensor Flow源代码，将内建的卷积计算方式更换为使用纹理内存的直接方式后，取得了较为明显的提升且网络准确度仍然维持在较高的水平，然而由于这种方式局限大，且更改源码需要重新编译、安装。在数据精度方面，使用FP16/INT8代替FP32并不会对网络总体准确度带来太大的下降，而在训练速度上提升很明显，实际应用中在准确度要求不高的情况下可以考虑用低精度数据替换；网络结构方面，将卷积核大小改为适合Tensor Core计算的形状能给训练速度带来一定提升，但是会极大降低网络准确度。最后，本文使用TensorRT对训练好的模型进行推理，在延迟方面有40\%-50\%的提升，故涉及到需要部署在设备上、对响应时间有要求的应用，应使用TensorRT进行优化。
\par 通过以上实验，可以总结出新架构的确能在特定情况下为机器学习中大量存在的矩阵混合运算带来明显的性能提升，从而提升总体机器学习应用的性能，然而目前为止，硬件仍然对输入、结构等较为敏感。且有些情况下仍然有性能更高的传统方式。所以在实际编码时，应根据问题规模、算法、结构、数据分布等方面合适选择不同方法，而不是一味使用新硬件提供的方法。
\subsection{展望}
\par 根据实验中发现的一些问题，本文也提出了关于指令、硬件运行机制等方面的猜想。
\subparagraph{更细粒度的同步机制}
\par 在矩阵乘加、卷积运算的实验中发现，上下文切换、内核的开启仍然占据了可观的时间，且这部分时间均为“死时间”。这与目前CUDA应用中最小同步粒度为线程束以及线程束内部线程分化有关。目前，线程束的同步在CUDA代码层面使用\_\_syncthread()、PTX和SASS代码层面使用bar指令实现。根据目前的实验现象可以推测，在未来的安培架构中将会出现线程级别的同步机制，为了协调资源的竞争使用，也许会使用$ Arrive-Wait $机制进行管理。
\subparagraph{规模更大的矩阵乘加指令}
\par 在矩阵乘加的实验中发现，wmma指令能极大缩短矩阵乘加应用在设备同步中花 费的时间。这也意味着矩阵乘加这一运算有着较强的并行可扩展性，即随着硬件、线程束的增加，其性能提升明显\cite{SCALABLITY}。这就意味着在下一代硬件中很有可能出现更大规模的跨线程束的矩阵乘加指令，由于本质上还是矩阵乘加运算，所以很可能计算硬件仍然延续本代架构的实现；当然，更大规模的指令也就意味着更多的线程束协调、合作和同步。
\subparagraph{针对稀疏矩阵优化的矩阵乘加指令}
\par 在支持向量机一节的实验发现，在某些问题中针对稀疏矩阵优化的API能取得优于当前张量核心API的性能，那么在下一代指令集中便很有可能结合两者优势，在张量核心相关的指令中加入针对稀疏矩阵优化的操作，以获得更强性能。由于本质上还是矩阵乘加运算，所以猜测加入的形式为在原有hmma指令的指令具体操作中进行扩展。
\par 以上设想、猜想是否会实现、或在一定程度上实现，只能交给时间去判断，这里仅仅提出一种想法供交流、讨论。
