\section{引言}

\subsection{研究背景}
\par 近年来，人工智能在全球无论是否是计算机相关行业中，都掀起了一股热潮，尤其是深度学习更是赚足了眼球。作为深度学习应用中计算能力支撑的并行计算硬件与软件更是迅猛发展，而英伟达(Nvidia)更是在并行硬件领域独占鳌头。
\par 在2017年，英伟达发布了一款基于伏特架构(Volta)的面向深度学习的GPU，Tesla V100\parencite{TESLAV100}，其中搭载了一些实验性的新技术；之后在2018年第三季度，英伟达发布了新一代图灵架构(Turing)，在该架构中，正式引入了许多革命性的新技术，同时也对原有技术做了很大的改进。有面向深度神经网络应用的张量核心(Tensor Core)\parencite{TENSORCORE}，能够大幅加速在神经网络训练、推理中的混合精度矩阵计算，该核心最先实验性地搭载于Tesla V100，在图灵架构中上至面向深度学习推理的Tesla T4，下至面向游戏玩家的RTX 2080Ti都搭载了这款核心；用于更高效搭建分布式计算平台的第二代端对端互联总线(NV Link 2.0)\parencite{NVLINK2}，相对于原有的QPI等总线，该总线能够直接互连GPU，且提供远高于原先SLI技术所能提供的带宽；以及针对游戏玩家推出的实时光线追踪技术(RTX)，该技术不在本文的讨论范围内。同时，由于GPU中CUDA计算单元架构包括流多处理器，纹理单元等的优化，在性能大幅提升的同时，热设计功耗(TDP)仍然维持在了上一代硬件的250W。
\par 在并行软件方面，与硬件一起，英伟达将其面向并行程序开发的SDK CUDA的版本更新到了10.0，在游戏应用、通用计算方面针对新架构的特性进行了优化；同时发布了基于CUDA 10.0的进行线性代数计算的模板库CUTLASS(CUDA Template Linear Algebra Subroutines)\parencite{CUTLASS}以利用其张量核心进行高效的代数运算。
\par 然而，官方文档给出的性能提升仅仅包括单一模块的理论性能提升，如传统CUDA核心的浮点数值计算的理论峰值，新加入的张量核心的混合矩阵计算(GEMM)的理论峰值；NV Link 2.0的理论峰值带宽等。在实际使用中，用户反映在网络推理方面以及基于支持新硬件的相关框架开发的机器学习应用中，提升并没有官方白皮书给出的的9倍之多\parencite{VOLTAWHITEPAPER}，且同类型不同规模应用的性能提升幅度并不一致，性能提升对神经网络中参数数量、网络层数等因素较为敏感。实际上，官方给出的文档中的提升也仅为绝对计算性能的提升，没有考虑应用类型、平台构建等条件。且目前关于新架构GPU的研究主要集中在大型计算节点的扩展效率 \parencite{EXASCLEDL}，基于GPGPU-Sim模拟的性能考察等\parencite{MODELING}，这些研究或是停留在表征性能层面、没有深入到代码或是中间代码层面；或是使用模拟技术、在PC机上进行模拟，尽管目前对于硬件的模拟运行的匹配度能够达到较高的水准，但是仍然有一定偏差，目前GPGPU-Sim的稳定版支持的最高的CUDA SDK版本为4.0，开发版本支持的最高的CUDA SDK版本为9.2。本文将直接针对真实的，单一的，图灵架构的GPU：RTX 2080Ti进行深入，结合版本最新的CUDA SDK 10.0以及对应的软件库包含CUTLASS，CUBLAS等，从架构、PTX 中间代码层面、SASS机器码层面对GPU在使用GPU加速的机器学习应用中的性能以及性能提升进行研究和评估；根据研究和评估结果以及分析得到的原因对现有CUDA代码进行优化；且将结合目前对于新老架构的对比研究\parencite{GRAVITATIONAL}，将新架构与麦克斯韦架构的GPU：GTX Titan X与帕斯卡架构的GPU：GTX 1080Ti进行横向对比，从实际替换成本、环境搭建成本、维护成本、性能/功耗比等角度对新架构进行评估与进一步设想。
\par 在最近刚结束的GTC 2019会议中，Nvidia发布了若干面向机器学习的硬件、软件。包括专为张量计算设计的Turing Tensor Core GPU，嵌入式平台的Jetson Nano\parencite{JETSONNANO}，将机器学习相关计算库整合起来的CUDA X\parencite{CUDAX}，这些都将在后文提到，但由于这些本质上都基于目前的Turing架构，故不会单独进行详细地说明。

\subsection{我们的工作}
\par 近年来，机器学习尤其是深度学习发展迅猛，各种方便程序员搭建模型的框架层出不穷。考虑到机器学习应用的计算量要求日益攀升，这些框架都陆续推出了基于GPU的版本。为方便程序员搭建模型，框架本身对硬件的操作进行了抽象。然而，正是因为这一层抽象，忽略了许多硬件层面的细节，使得框架无法完全利用硬件的性能。这也导致了许多用户反映在实际应用中，新架构的性能提升并没有官方给出的文档数值、硬件参数(包括流处理器、纹理/光栅单元)、甚至价格上涨幅度那么多。
\par 为了尽可能在实际应用场景中提升硬件性能的利用率，本文将从如下层面对基于CUDA以及相关框架的机器学习应用进行研究与评估；挖掘理论与实际不符的原因；并做出适当的修改和建议。
\begin{itemize}
	\item CUDA源码
	\item CUDA源码编译出的PTX中间代码
	\item 基于CUDA的框架的应用源码
\end{itemize}
\par 因CUDA SDK 10.0发布不久，目前许多框架还未对该SDK进行相关优化；一些既存的CUDA应用仍是基于CUDA SDK 9甚至CUDA SDK 8进行编译的。所以本文将结合对于上述三个层面的分析结果，结合新硬件、新架构、新SDK的特征，在源码层面进行调整并给出一些编写相关程序时的建议；力图尽可能多地发掘新硬件、新架构的潜力。

\subsection{本文的组织结构}
\par 本文在第2章中介绍了该研究的背景和相关的工作。首先介绍了基于GPU的机器学习应用与CUDA的相关背景知识。由介绍基于GPU的机器学习应用引出CUDA的相关介绍，包括CUDA应用的编程模型、编译过程、调用/执行方式。然后介绍了目前对于评估、模拟GPU，尤其是CUDA应用性能开展的相关工作；由超微半导体(AMD)开发的GPU也具有通用计算功能，然而目前市面上还没有基于AMD开发的GPU的相关SDK或框架，故本文不做讨论。最后介绍了基于CUDA的可执行程序的汇编代码结构和使用CUDA源码编译得到的PTX中间代码的结构，供之后的分析使用。
\par 本文在第3章中首先简要介绍实验动机、实验步骤以及实验结果。然后介绍了实验所需的工具、环境以及搭建方式等。接着详细介绍了我们的主要工作，包括基于单一功能、测试用的应用的Benchmark、基于CUDA源码的机器学习应用的研究过程、针对汇编代码与PTX中间代码的研究过程、针对基于CUDA的相关框架的机器学习应用的研究过程以及根据分析得出的结果给出的修改、建议等。最后给出了各项实验的结果和对比，并进一步分析原因。
\par 本文最后在第4章进行总结，并给出之后改进与深入工作的设想和预期。

