\clearpage                                          %       


\setcounter{page}{1} 
\pagenumbering{Roman}% Roman page numbers                             %
\input{paper_info.tex}                          %
\renewcommand\abstractname{\bfseries \heiti \zihao{3}摘\hspace{1cm}要}  %
\addcontentsline{toc}{section}{摘要}
\begin{abstract}\zihao{5}\songti                    %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\vspace{3em}
	\par 本文主要针对NVIDIA新架构的GPU(图灵架构)为机器学习应用带来的性能提升进行研究，由于目前实际使用中的应用很难达到NVIDIA官方宣传的性能提升幅度，故本文将从应用类型、代码结构结合硬件、指令特征对这一现象进行研究，并提出相应的建议。
	\par 实验采用自底向上结构，使用定量方法，通过不同世代的硬件与SDK进行横向比较，以及同一世代硬件与SDK和不同类型应用进行纵向比较研究新架构硬件的特点，而硬件中的张量核心是本文的重点实验对象。
	\par 首先，实验从底层的基于性能评估的Benchmark出发，考察了矩阵乘加、矩阵乘法、卷积运算的性能，并从指令、硬件机制等方面进行了分析，作为基准。在这基础上，借助基于CUDA C++源码的卷积神经网络与最小序列优化支持向量机，考察了若干模块整合后应用的总体性能，从调用、运行情况对结果进行分析，并从稀疏矩阵、纹理内存等方面尝试优化。最后，结合既得的实验结论，对基于Tensor Flow-GPU框架的简单卷积神经网络进行评估，并在框架的底层源码层面从卷积方式、纹理内存、超参数、数据精度等方面进行优化。除神经网络的训练外，实验还对推理过程进行评估，并使用TensorRT以及对应的硬件Jetson进行加速。
	\par 文章的最后，根据实验结果对指令集、同步机制的下一步发展做出了合理的展望。
	\newline
	\newline
	{\bfseries \sffamily\zihao{5} 关键词：} \zihao{5}{\rmfamily \KeywordsCHS}
\end{abstract}                                                    %
%通常你不需要修改这部分内容
\clearpage                         
\renewcommand\abstractname{\bfseries \zihao{3}Abstract}  
\addcontentsline{toc}{section}{Abstract}                 %
\begin{abstract}\zihao{5}                                         %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\vspace{3em}
	\par This paper is focusing on the performance improvement in Machine Learning application brougnt by Nvidia’s new architecture (Turing architecture) GPU. Since currently the Machine Learning application actually in used can hardly get as much improvement as mentioned in Nvidia’s official White Paper, so, this paper will research this situation through the type of the application, the structure of the source code combining with feature of the hardware and instructions, thus give corresponding recommendation about coding. This paper uses quantitative methods, doing both horizontal comparation with hardware and SDK of different generations and vertical comparation with different types of problem running on the same generation of hardware and SDK, through which the pattern and feature can be extracted. Among all the new features, the most important is Tensor Core and corresponding library CUTLASS (CUDA Template Linear Algebra Subroutine), this paper evaluate this unit through GEMM, Matrix Multiple, Convolution, etc. Also, traditional matrix library CUBLAS, optimizer TensorRT, Float Point and GRAM are also mentioned.  
	\par In the conclusion, Tensor Core in the new architecture GPU is very sensitive to the type of applications, type of calculations, meta parameter, etc., to achieve expected performance, the scale of the data, shape of the data and type of calculations should be well fit to the hardware. Moreover, in some situation including the input matrixs are sparse and the scale of the input data is small, library oriented to sparse matrix (CUSPARSE) and methods based on texture memory will gain much higher performance, and in situation that the input fit the hardware well, the Tensor Core can bring the application a significant improvement in performance. When it comes to the inference stage, TensorRT can bring a significant improvement in almost all the situation. 
	\par So, in the training stage of actual application, the usage of hardware, SDK, memory, etc. should be chosen appropriate based on the feature of the applications, and in the inference stage, do not hesitate to use TensorRT!
	\newline
	\newline
	{\bfseries \zihao{5} Keywords：} {\zihao{5} \KeywordsENG}        %   
\end{abstract}                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%